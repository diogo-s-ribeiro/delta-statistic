{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<body>\n",
    "  <b><p style=\"color:#FF0000\";>0. Python version conflicts</p></b>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; The new Python versions (>3.12) have some conflicts with some of the necessary packages. If possible, run this code in a Python=3.11. </p>\n",
    "<p> &ensp; To avoid dependency issues in your main Python environment, it's safer to use Conda, which helps prevent unnecessary future conflicts in your base environment.</p>\n",
    "<p> &ensp; To streamline the setup process, I have created a Conda environment that includes all the necessary packages.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -f ./env/delta_env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can create your own Conda environment and manually install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n delta_env python=3.11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way, don't forget to activate the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate delta_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<body>\n",
    "  <b><p style=\"color:#FF0000\";>1. Prepare Directory</p></b>\n",
    "</body>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; Before you start, make sure git is installed in you PC. If not, you can skip this step and just manually transfer the files into your current working directory. </p>\n",
    "<p> &ensp; Transfer the files with git to your computer. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/diogo-s-ribeiro/delta-statistic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; Move to the correct directory. Again, you can just manually open python directly in the \"Delta-Python\" folder. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./delta-statistic/Delta-Python/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<body>\n",
    "  <b><p style=\"color:#FF0000\";>2. Install and import Packages that are needed in this new environment</p></b>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; Make sure your python version is up-to-date. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; You can now install packages and libraries specific to this environment using \"pip\". </p>\n",
    "<p> &ensp; If you only want to run a specific part of the code, you can install it one-by-one, otherwise it's simpler to run in the terminal: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; Now simply import the needed packages to run the code: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from numba import njit, float64, int64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<body>\n",
    "  <b><p style=\"color:#FF0000\";>3. Import the delta-statistic (python) code</p></b>\n",
    "</body>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; The delta function calculates the delta-statistic after an MCMC step. It uses the emcmc function to obtain Markov chain samples and then calculates the delta statistic. The [@njit] decorator is used for just-in-time compilation, which can improve the performance of the code.  </p>\n",
    "<p> &ensp; To run it, first import the delta-statistic code available in Python. </p>\n",
    "\n",
    "<br>\n",
    "<details><summary> References </summary>\n",
    "\n",
    "Borges, R. et al. (2019). Measuring phylogenetic signal between categorical traits and phylogenies. Bioinformatics, 35, 1862-1869.<br>\n",
    "[Article link](https://doi.org/10.1093/bioinformatics/bty800)\n",
    "<br><br>\n",
    "Diogo, R. (Github). Assessing traits and phylogenetic signal to unravel the tempo and mode of phenotypic evolution.<br>\n",
    "[link](https://github.com/diogo-s-ribeiro/delta-statistic)\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "<p> &ensp; This can be done either by: </p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                3A. Importing from file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> Importing the provided delta-statistic file, if it is in the same working directory; </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta_functs import delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                3B. Directly (Code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> Copying all of the delta-statistic related functions. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings step for alpha parameter\n",
    "@njit(float64(float64, float64, float64[::1], float64, float64))\n",
    "def mhalpha(a,b,x,l0,se):\n",
    "    '''a = The current value of the alpha parameter.\n",
    "    b    = The current value of the beta parameter.\n",
    "    x    = An array of data points used in the acceptance ratio computations, after uncertainty is calculated.\n",
    "    l0   = A constant value used in the acceptance ratio computations.\n",
    "    se   = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.'''\n",
    "\n",
    "    a1   = np.exp(np.random.normal(np.log(a),se, 1))[0]\n",
    "    lp_a = np.exp( (len(x)*(math.lgamma(a1+b)-math.lgamma(a1)) - a1*(l0-np.sum(np.log(x)))) - (len(x)*(math.lgamma(a+b)-math.lgamma(a)) - a*(l0-np.sum(np.log(x)))) )\n",
    "    r    = min( 1, lp_a ) \n",
    "\n",
    "    # Repeat until a valid value is obtained\n",
    "    while (np.isnan(lp_a) == True):\n",
    "        a1   = np.exp(np.random.normal(np.log(a),se, 1))[0]\n",
    "        lp_a = np.exp( (len(x)*(math.lgamma(a1+b)-math.lgamma(a1)) - a1*(l0-np.sum(np.log(x)))) - (len(x)*(math.lgamma(a+b)-math.lgamma(a)) - a*(l0-np.sum(np.log(x)))) )\n",
    "        r    = min( 1, lp_a )\n",
    "    \n",
    "    # Accept or reject based on the acceptance ratio\n",
    "    if np.random.uniform(0,1) < r:\n",
    "        return a1\n",
    "    else:\n",
    "        return a\n",
    "\n",
    "\n",
    "# Metropolis-Hastings step for beta parameter\n",
    "@njit(float64(float64, float64, float64[::1], float64, float64))\n",
    "def mhbeta(a,b,x,l0,se):\n",
    "    '''a = The current value of the alpha parameter.\n",
    "    b    = The current value of the beta parameter.\n",
    "    x    = An array of data points used in the acceptance ratio computations, after uncertainty is calculated.\n",
    "    l0   = A constant value used in the acceptance ratio computations.\n",
    "    se   = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.'''\n",
    "    \n",
    "    b1   = np.exp(np.random.normal(np.log(b),se,1))[0]\n",
    "    lp_b = np.exp( (len(x)*(math.lgamma(a+b1)-math.lgamma(b1)) - b1*(l0-np.sum(np.log(1-x)))) - (len(x)*(math.lgamma(a+b)-math.lgamma(b)) - b*(l0-np.sum(np.log(1-x)))) )\n",
    "    r    = min( 1, lp_b )\n",
    "    \n",
    "    # Repeat until a valid value is obtained\n",
    "    while (np.isnan(lp_b) == True):\n",
    "        b1   = np.exp(np.random.normal(np.log(b),se,1))[0]\n",
    "        lp_b = np.exp( (len(x)*(math.lgamma(a+b1)-math.lgamma(b1)) - b1*(l0-np.sum(np.log(1-x)))) - (len(x)*(math.lgamma(a+b)-math.lgamma(b)) - b*(l0-np.sum(np.log(1-x)))) )\n",
    "        r    = min( 1, lp_b )\n",
    "    \n",
    "    # Accept or reject based on the acceptance ratio\n",
    "    if np.random.uniform(0,1) < r:\n",
    "        return b1\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "\n",
    "# Metropolis-Hastings algorithm using alpha and beta\n",
    "@njit(float64[:, ::1](float64, float64, float64[::1], float64, float64, int64, int64, int64))\n",
    "def emcmc(alpha,beta,x,l0,se,sim,thin,burn):\n",
    "    '''alpha = The initial value of the alpha parameter.\n",
    "    beta     = The initial value of the beta parameter.\n",
    "    x        = An array of data points used in the acceptance ratio computations, after uncertainty is calculated.\n",
    "    l0       = A constant value used in the acceptance ratio computations.\n",
    "    se       = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.\n",
    "    sim      = The number of total iterations in the Markov Chain Monte Carlo (MCMC) simulation.\n",
    "    thin     = The thinning parameter, i.e., the number of iterations to discard between saved samples.\n",
    "    burn     = The number of burn-in iterations to discard at the beginning of the simulation.'''\n",
    "\n",
    "    n_size = np.linspace(burn, sim, int((sim - burn) / thin + 1))\n",
    "    usim   = np.round(n_size, 0, np.empty_like(n_size))\n",
    "    gibbs  = []\n",
    "    p      = 0\n",
    "\n",
    "    for i in range(sim+1):\n",
    "        alpha = mhalpha(alpha,beta,x,l0,se)\n",
    "        beta  = mhbeta(alpha,beta,x,l0,se)\n",
    "        \n",
    "        if i == usim[p]:\n",
    "            gibbs.append((alpha, beta))\n",
    "            p += 1\n",
    "            \n",
    "    gibbs = np.asarray(gibbs)      \n",
    "    return gibbs\n",
    "\n",
    "\n",
    "# Calculate uncertainty using different types\n",
    "def entropy_type(prob, ent_type):\n",
    "    '''prob  = A matrix of ancestral probabilities.\n",
    "    ent_type = A string indicating the type of entropy calculation. (options: 'LSE', 'SE', or any other value for Gini impurity).'''\n",
    "    \n",
    "    # Linear Shannon Entropy\n",
    "    if ent_type == 'LSE':\n",
    "        k    = np.shape(prob)[1]\n",
    "        prob = np.asarray(np.where(prob<=(1/k), prob, prob/(1-k) - 1/(1-k)))\n",
    "        tent = np.sum(prob, 1)\n",
    "        \n",
    "        # Ensure absolutes\n",
    "        tent = np.asarray(np.where(tent != 0, tent, tent + np.random.uniform(0,1,1)/10000))\n",
    "        tent = np.asarray(np.where(tent != 1, tent, tent - np.random.uniform(0,1,1)/10000))\n",
    "        \n",
    "        return tent\n",
    "\n",
    "    # Shannon Entropy\n",
    "    elif ent_type == 'SE':\n",
    "        k    = np.shape(prob)[1]\n",
    "        tent = entropy(prob, base=k, axis=1)\n",
    "        \n",
    "        # Ensure absolutes\n",
    "        tent = np.asarray(np.where(tent != 0, tent, tent + np.random.uniform(0,1,1)/10000))\n",
    "        tent = np.asarray(np.where(tent != 1, tent, tent - np.random.uniform(0,1,1)/10000))\n",
    "\n",
    "        return tent\n",
    "\n",
    "    # Ginni Impurity\n",
    "    else:\n",
    "        k    = np.shape(prob)[1]\n",
    "        tent = ((1 - np.sum(prob**2, axis=1))*k)/ (k - 1)\n",
    "        \n",
    "        # Ensure absolutes\n",
    "        tent = np.asarray(np.where(tent != 0, tent, tent + np.random.uniform(0,1,1)/10000))\n",
    "        tent = np.asarray(np.where(tent != 1, tent, tent - np.random.uniform(0,1,1)/10000))\n",
    "\n",
    "        return tent\n",
    "\n",
    "\n",
    "# Calculate delta-statistic after an MCMC step\n",
    "def delta(x,lambda0,se,sim,thin,burn,ent_type):\n",
    "    '''x     = A matrix of ancestral probabilities.\n",
    "    lambda0  = A constant value used in the acceptance ratio computations.\n",
    "    se       = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.\n",
    "    sim      = The number of total iterations in the Markov Chain Monte Carlo (MCMC) simulation.\n",
    "    thin     = The thinning parameter, i.e., the number of iterations to discard between saved samples.\n",
    "    burn     = The number of burn-in iterations to discard at the beginning of the simulation.\n",
    "    ent_type = A string specifying the type of entropy calculation (options: 'LSE', 'SE', or any other value for Gini impurity).'''\n",
    "    \n",
    "    mc1    = emcmc(np.random.exponential(),np.random.exponential(),entropy_type(x, ent_type),lambda0,se,sim,thin,burn)\n",
    "    mc2    = emcmc(np.random.exponential(),np.random.exponential(),entropy_type(x, ent_type),lambda0,se,sim,thin,burn)\n",
    "    mchain = np.concatenate((mc1,mc2), axis=0)\n",
    "    \n",
    "    deltaA = (np.mean(mchain[:,1]))/(np.mean(mchain[:,0]))\n",
    "    \n",
    "    return deltaA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<body>\n",
    "  <b><p style=\"color:#FF0000\";>4. Ancestral Probabilities</p></b>\n",
    "</body>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; The ancestral probabilities are needed before calculating their respective delta-statistic. This can be done either through Maximum Likelihood or Bayesian inference. </p>\n",
    "<p> &ensp; Multiple software packages are currently available that can perform ancestral state reconstruction. Choose the best suited for your needs! </p>\n",
    "\n",
    "<br>\n",
    "<details><summary> Reference </summary>\n",
    "\n",
    "Joy,J.B. et al. (2016) Ancestral reconstruction. PLOS Computational Biology, 12.<br>\n",
    "[Article link](https://doi.org/10.1371/journal.pcbi.1004763)\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "<p> &ensp; Since only the matrix with ancestral probabilities is needed, you can: </p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4A. Imput them directly from a file, after calculating them through an external source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_ace(file_path, separator=',', rmv_col_name=True, rmv_row_name=True):\n",
    "    ''' file_path = Represents the path to the file that you want to read.\n",
    "    separator     (default: ',')  = Specifies the separator used in the file to separate the values.\n",
    "    rmv_col_name  (default: True) = Boolean value that determines whether the first row (column names) should be removed from the array.\n",
    "    rmv_row_name  (default: True) = Boolean value that determines whether the first column (index) should be removed from the array.'''\n",
    "\n",
    "    # Read the data from the file using numpy's genfromtxt function\n",
    "    array = np.genfromtxt(file_path, delimiter = separator)\n",
    "    \n",
    "    # Check if the column name should be removed\n",
    "    if rmv_col_name == True:\n",
    "        array = np.delete(array, 0, axis=0)     # Remove the first row (Trait Name)\n",
    "    \n",
    "    # Check if the row name should be removed \n",
    "    if rmv_row_name == True:\n",
    "        array = np.delete(array, 0, axis=1)     # Remove the first column (Entity Name)\n",
    "        \n",
    "    return array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4B. Or use a package to calculate them directly in Python (e.g.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                4B.1. PastML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; A good package for Ancestral Character Estimation (through Maximum Likelihood) in Python is PastML.</p>\n",
    "\n",
    "<br>\n",
    "<details><summary> Reference </summary>\n",
    "\n",
    "Ishikawa, S. A. et al. (2019). A fast likelihood method to reconstruct and visualize ancestral scenarios. Molecular Biology and Evolution, 36, 2069-2085.<br>\n",
    "[Article link](https://doi.org/10.1093/molbev/msz131)\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "<p> &ensp; If you intend to run PastML don't forget to install the package. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pastml.tree import read_tree, name_tree\n",
    "from pastml.acr import acr\n",
    "from pastml.annotation import preannotate_forest\n",
    "from pastml import col_name2cat\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_input(tree_nwk, data, data_sep=',', single_tree_file=False):\n",
    "    '''tree_nwk      = Represents the path to the Newick file containing the tree or a string with the tree itself.\n",
    "    data             = Represents the path to the data file or DataFrame used for annotation with leaf states.\n",
    "    data_sep         (default: ',')   = Separator used in the data file.\n",
    "    single_tree_file (default: False) = Boolean value that specifies whether the input tree is provided as a single file.'''\n",
    "    \n",
    "    if single_tree_file==False:\n",
    "        with open(tree_nwk, 'r') as f:                                                 # Reads the tree from a Newick file and returns its roots\n",
    "            nwks = f.read().replace('\\n', '')\n",
    "        roots = [read_tree(tree_nwk)]\n",
    "    else:\n",
    "        roots = [read_tree(tree_nwk)]                                                  # Reads the newick tree and returns its roots\n",
    "\n",
    "    column2annotated = Counter()                                                       # Counter to keep track of the number of times each column is annotated\n",
    "    column2states    = defaultdict(set)                                                # Dictionary to store the unique states for each column\n",
    "\n",
    "    # Read the data as a pandas DataFrame\n",
    "    df         = pd.read_csv(data, sep=data_sep, index_col=0, header=0, dtype=str)\n",
    "    df.index   = df.index.map(str)\n",
    "    df.columns = [col_name2cat(column) for column in df.columns]\n",
    "    columns    = df.columns\n",
    "    \n",
    "    node_names     = set.union(*[{n.name for n in root.traverse() if n.name} for root in roots])     # Get the names of the nodes in the tree\n",
    "    df_index_names = set(df.index)                                                                   # Get the index names from the DataFrame\n",
    "    common_ids     = list(node_names & df_index_names)                                               # Find the common IDs between node names and DataFrame index names\n",
    "    \n",
    "    # strip quotes if needed\n",
    "    if not common_ids:\n",
    "        node_names = {_.strip(\"'\").strip('\"') for _ in node_names}\n",
    "        common_ids = node_names & df_index_names\n",
    "        if common_ids:\n",
    "            for root in roots:\n",
    "                for n in root.traverse():\n",
    "                    n.name = n.name.strip(\"'\").strip('\"')\n",
    "\n",
    "    # Preannotate the forest with the DataFrame\n",
    "    preannotate_forest(roots, df=df)\n",
    "\n",
    "    # Populate the column2states dictionary with unique states for each column\n",
    "    for c in df.columns:\n",
    "        column2states[c] |= {_ for _ in df[c].unique() if pd.notnull(_) and _ != ''}\n",
    "\n",
    "    num_tips = 0\n",
    "\n",
    "    # Count the number of annotated columns for each node\n",
    "    column2annotated_states = defaultdict(set)\n",
    "    for root in roots:\n",
    "        for n in root.traverse():\n",
    "            for c in columns:\n",
    "                vs = getattr(n, c, set())\n",
    "                column2states[c] |= vs\n",
    "                column2annotated_states[c] |= vs\n",
    "                if vs:\n",
    "                    column2annotated[c] += 1\n",
    "            if n.is_leaf():\n",
    "                num_tips += 1\n",
    "\n",
    "    if column2annotated:\n",
    "        c, num_annotated = min(column2annotated.items(), key=lambda _: _[1])\n",
    "    else:\n",
    "        c, num_annotated = columns[0], 0\n",
    "\n",
    "    # Calculate the percentage of unknown tip annotations\n",
    "    percentage_unknown = (num_tips - num_annotated) / num_tips\n",
    "    if percentage_unknown >= .9:\n",
    "        raise ValueError('{:.1f}% of tip annotations for character \"{}\" are unknown, '\n",
    "                         'not enough data to infer ancestral states. '\n",
    "                         '{}'\n",
    "                         .format(percentage_unknown * 100, c,\n",
    "                                 'Check your annotation file and if its ids correspond to the tree tip/node names.'\n",
    "                                 if data\n",
    "                                 else 'You tree file should contain character state annotations, '\n",
    "                                      'otherwise consider specifying a metadata file.'))\n",
    "    c, states = min(column2annotated_states.items(), key=lambda _: len(_[1]))\n",
    "\n",
    "    # Check if the number of unique states is too high for the given number of tips\n",
    "    if len(states) > num_tips * .75:\n",
    "        raise ValueError('Character \"{}\" has {} unique states annotated in this tree: {}, '\n",
    "                         'which is too much to infer on a {} with only {} tips. '\n",
    "                         'Make sure the character you are analysing is discrete, and if yes use a larger tree.'\n",
    "                         .format(c, len(states), states, 'tree' if len(roots) == 1 else 'forest', num_tips))\n",
    "\n",
    "\n",
    "    # Convert column2states to numpy arrays and sort the states\n",
    "    column2states = {c: np.array(sorted(states)) for c, states in column2states.items()}\n",
    "\n",
    "    # Name the trees in the forest\n",
    "    for i, tree in enumerate(roots):\n",
    "        name_tree(tree, suffix='' if len(roots) == 1 else '_{}'.format(i))\n",
    "\n",
    "    return roots, columns, column2states\n",
    "\n",
    "\n",
    "def marginal(tree, data, prediction_method='MPPA', model='F81', threads=0, single_tree_file=False):\n",
    "    '''tree           = Represents the path to the Newick file containing the tree or a string with the tree itself.\n",
    "    data              = Represents the path to the data file or DataFrame used for annotation with leaf states.\n",
    "    prediction_method (default: 'MPPA') = Specifies the ancestral character prediction method.\n",
    "    model             (default: 'F81')  = Specifies the evolutionary model used for reconstruction.\n",
    "    threads           (default: 0)      = Specifies the number of threads to use for the analysis.\n",
    "    single_tree_file  (default: False)  = Boolean value that specifies whether the input tree is provided as a single file.'''\n",
    "\n",
    "    # Set the number of threads based on the available CPU cores\n",
    "    if threads < 1:\n",
    "        threads = max(os.cpu_count(), 1)\n",
    "\n",
    "    # Validate the input and get the roots, columns, and column2states\n",
    "    roots, columns, column2states = \\\n",
    "        _validate_input(tree_nwk=tree, data=data, data_sep=',', single_tree_file=single_tree_file)\n",
    "    \n",
    "    # Perform the ancestral character reconstruction (ACR) analysis\n",
    "    acr_results = acr(forest=roots, columns=columns, column2states=column2states, prediction_method=prediction_method, model=model, threads=threads)\n",
    "\n",
    "    # Get the leaf names from the tree\n",
    "    leaf_names = read_tree(tree).get_leaf_names()\n",
    "\n",
    "    # Get the marginal probabilities and exclude the leaf nodes\n",
    "    marginal   = np.asarray( acr_results[0]['marginal_probabilities'].drop(leaf_names) )\n",
    "\n",
    "    return marginal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    4B.2. rpy2:ape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; Another popular option to calculate the marginal probabilities would be to use the ape package in Python through rpy2. </p>\n",
    "\n",
    "<br>\n",
    "<details><summary> References </summary>\n",
    "\n",
    "Paradis, E. and Schliep, K. (2019). ape 5.0: an environment for modern phylogenetics and evolutionary analyses in R. Bioinformatics, 35, 526-528<br>\n",
    "[Article link](https://doi.org/10.1093/bioinformatics/bty633)\n",
    "<br><br>\n",
    "Rpy2 Package<br>\n",
    "[link](https://rpy2.github.io)\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "<p> &ensp; Once again, don't forget to install the necessary packages.</p>\n",
    "<small> <p> &ensp; PS. If the ancestral probabilities matrix can't properly be calculated, resulting in a matrix with multiple -<b>NaN</b>- values, there might be a problem related to the tree branch lengths. Try uncommenting the \"tree$edge.length\" line and rerun the code. </small> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ape(dir_tree, dir_data, instal_ape=False, tree_dir=False):\n",
    "    '''dir_tree = Represents the path to the Newick file containing the tree or a string with the tree itself.\n",
    "    dir_data    = Represents the path to the data file or DataFrame used for annotation with leaf states.\n",
    "    instal_ape  (default: False) = Boolean value that determines whether to install the 'ape' package or not.\n",
    "    tree_dir    (default: False) = Boolean value that specifies whether the dir_tree variable should be used to load the tree data from a file.'''\n",
    "\n",
    "    traits_df = pd.read_csv(dir_data, sep=',', index_col=0, header=0, dtype=str).sort_index().iloc[:,0]                             # Reading traits data from a CSV file and sorting it\n",
    "    traits    = tuple(  traits_df.replace( np.sort(np.unique(traits_df)), list(range( 1, 1+len(np.unique(traits_df)))))  )          # Replacing unique traits with numerical values\n",
    "\n",
    "    # Loading tree data from a file\n",
    "    if tree_dir==True:\n",
    "        dir_tree = np.loadtxt( dir_tree , dtype=str)\n",
    "\n",
    "    # The R code installs the 'ape' package if \"instal_ape\"=True\n",
    "    if instal_ape == True:\n",
    "        instal_ape = ''\n",
    "    else:\n",
    "        instal_ape = '#'\n",
    "\n",
    "    marginal_prob  = r(\n",
    "        \"\"\"\n",
    "        # Import Ape\n",
    "        {instal_ape}install.packages('ape')\n",
    "        library('ape')\n",
    "\n",
    "        # Tree Import and transform\n",
    "        tree_newick <- read.tree(text=\"{dir_tree}\")\n",
    "        tree <- multi2di( tree_newick )\n",
    "        tree$edge.length <- tree$edge.length + runif(tree$edge.length, 0, 1e-7)\n",
    "        \n",
    "        # Trait vector import and order vector correctly depending on the tree\n",
    "        trait <- c{traits}\n",
    "        trait <- trait[ rank( tree$tip.label ) ]\n",
    "\n",
    "        # Marginal Probabilities calculate\n",
    "        ar <- ace(trait,tree,type=\"discret\",method=\"ML\",model=\"ARD\")$lik.anc\n",
    "\n",
    "        \"\"\".format(instal_ape=instal_ape, dir_tree=dir_tree, traits=traits)\n",
    "    )\n",
    "\n",
    "    return marginal_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<body>\n",
    "  <b><p style=\"color:#FF0000\";>5. Calculate Delta-Statistic</p></b>\n",
    "</body>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Delta and Uncertainty variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0  = 0.1                       # rate parameter of the proposal\n",
    "se       = 0.5                       # standard deviation of the proposal\n",
    "sim      = 100000                    # number of iterations\n",
    "thin     = 10                        # Keep only each xth iterate\n",
    "burn     = 100                       # Burned-in iterates\n",
    "\n",
    "ent_type = 'LSE'                     # Linear Shannon Entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; It is also possible to calculate uncertainty using a normalized version of both: </p>\n",
    "<ul>\n",
    "  <li> The Shannon Entropy: &emsp; A widely used measure of information content or uncertainty in a random variable; </li>\n",
    "  <li> The Gini impurity: &emsp;&emsp;&emsp; Measure that can be used to quantify the impurity or disorder in a set of class labels. </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ACE (see above \"4. Ancestral Probabilities\") and Delta Calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &ensp; Before starting, make sure that: </p>\n",
    "<ul>\n",
    "  <li> The necessary files are in the correct directory; </li>\n",
    "  <li> The functions of the preferred ACE and delta calculation have already ran. </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                E.g. A1. (Single) Input directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ap      = r\"./input/Simplified/Ancestral_Probabilities/FILE\"       # Path to the (ap: ancestral probabilities) input files\n",
    "\n",
    "file         = \"Simplified_AP_1.txt\"                                    # File with Ancestral Probabilities\n",
    "file         = path_ap.replace('FILE', file)                            # Path + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6983661841354931\n"
     ]
    }
   ],
   "source": [
    "Ancest_Prob  = read_file_ace(file_path=file, separator=',', rmv_col_name=True, rmv_row_name=True)\n",
    "Delta_Final  = delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type='LSE')\n",
    "\n",
    "print( Delta_Final )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                E.g. A2. (Multiple) Input directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_files_ap( dir=None, separator=',', rmv_col_name=True, rmv_row_name=True, lambda0=0.1, se=0.5, sim=100000, thin=10, burn=100, ent_type='LSE' ):\n",
    "    '''dir       (default: None)   = Represents the directory path with the ancestral probabilities matrices. If no value is provided when calling the function, the user will be prompted to input the directory path.\n",
    "    separator    (default: ',')    = Determines the separator used in the matrices when reading the files.\n",
    "    rmv_col_name (default: True)   = Boolean value that indicates whether the column names should be removed when reading the files.\n",
    "    rmv_row_name (default: True)   = Boolean value that indicates whether the row names should be removed when reading the files.\n",
    "    lambda0      (default: 0.1)    = A constant value used in the acceptance ratio computations.\n",
    "    se           (default: 0.5)    = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.\n",
    "    sim          (default: 100000) = The number of total iterations in the Markov Chain Monte Carlo (MCMC) simulation.\n",
    "    thin         (default: 10)     = The thinning parameter, i.e., the number of iterations to discard between saved samples.\n",
    "    burn         (default: 100)    = The number of burn-in iterations to discard at the beginning of the simulation.\n",
    "    ent_type     (default: 'LSE')  = A string specifying the type of entropy calculation (options: 'LSE', 'SE', or any other value for Gini impurity).'''\n",
    "    \n",
    "    if dir == None:\n",
    "        dir = str(input( 'What is the directory path with the ancestral probabilities matrices?' ))\n",
    "\n",
    "    dic_files = set( os.listdir(dir) )\n",
    "    dic_delta = set()\n",
    "    for file in dic_files:\n",
    "        file = dir + file\n",
    "        ace = read_file_ace( file, separator, rmv_col_name, rmv_row_name )\n",
    "        dic_delta.add( delta(x=ace, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type=ent_type) )\n",
    "    \n",
    "    return dict(zip( dic_files, dic_delta ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Simplified_AP_1.txt': 0.02917667931331717, 'Simplified_AP_3.txt': 1.7437280098489205, 'Simplified_AP_2.txt': 1.0306355855677944}\n",
      "dict_values([0.02917667931331717, 1.7437280098489205, 1.0306355855677944])\n"
     ]
    }
   ],
   "source": [
    "Delta_Final_dic = multiple_files_ap( dir = './input/Simplified/Ancestral_Probabilities/' )\n",
    "\n",
    "print( Delta_Final_dic )\n",
    "print( Delta_Final_dic.values() )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                E.g. B1. (Single) PastML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data  = r\"./input/3Class/3C_States.txt\"                 # File containing tip/node annotations, in csv or tab format\n",
    "path_tree  = r\"./input/3Class/Trees/3C_Trees_1.txt\"          # File containing tip/node annotations, in csv or tab format\n",
    "\n",
    "method     = \"MPPA\"                                          # MPPA, MAP\n",
    "model      = \"F81\"                                           # F81, JC, EFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.372946463453959\n"
     ]
    }
   ],
   "source": [
    "Ancest_Prob = marginal(path_tree, path_data)\n",
    "Delta_Final = delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type='LSE')\n",
    "\n",
    "print(Delta_Final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                E.g. B2. (Multiple) PastML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_files_PastML( dir_tree=None, dir_data=None, single_tree_file=False, threads=0, lambda0=0.1, se=0.5, sim=100000, thin=10, burn=100, ent_type='LSE' ):\n",
    "    '''dir_tree      (default: None)   = Represents the directory path to the phylogenetic tree(s) file(s). If no value is provided when calling the function, the user will be prompted to input the directory path.\n",
    "    dir_data         (default: None)   = Represents the directory path to the observed states file(s). If no value is provided when calling the function, the user will be prompted to input the directory path.\n",
    "    single_tree_file (default: False)  = Boolean value that indicates whether the phylogenetic tree(s) file(s) are stored in a single file (True) or multiple files (False).\n",
    "    threads          (default: None)   = This variable represents the number of threads used in the marginal function.\n",
    "    lambda0          (default: 0.1)    = A constant value used in the acceptance ratio computations.\n",
    "    se               (default: 0.5)    = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.\n",
    "    sim              (default: 100000) = The number of total iterations in the Markov Chain Monte Carlo (MCMC) simulation.\n",
    "    thin             (default: 10)     = The thinning parameter, i.e., the number of iterations to discard between saved samples.\n",
    "    burn             (default: 100)    = The number of burn-in iterations to discard at the beginning of the simulation.\n",
    "    ent_type         (default: 'LSE')  = A string specifying the type of entropy calculation (options: 'LSE', 'SE', or any other value for Gini impurity).'''\n",
    "    \n",
    "    if dir_tree == None:\n",
    "        dir_tree = str(input( 'What is the directory path to the phylogenetic tree(s) file(s)?' ))\n",
    "    if dir_data == None:\n",
    "        dir_data = str(input( 'What is the directory path to the observed states file(s)?' ))\n",
    "\n",
    "    if single_tree_file==False:\n",
    "        dic_files = set( os.listdir( dir_tree ) )\n",
    "        dic_delta = set()\n",
    "        for phylo_tree in dic_files:\n",
    "            Ancest_Prob  = marginal( dir_tree+'/'+phylo_tree, dir_data, single_tree_file=single_tree_file, threads=threads )\n",
    "            dic_delta.add( delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type=ent_type) )\n",
    "        dic_final = dict(zip( dic_files, dic_delta ))\n",
    "        \n",
    "    else:\n",
    "        dic_final = {}\n",
    "        arr       = np.loadtxt( dir_tree , dtype=str)\n",
    "        indx      = 0\n",
    "        for phylo_tree in arr:\n",
    "            Ancest_Prob = marginal( phylo_tree, dir_data, single_tree_file=single_tree_file, threads=threads )\n",
    "            delta_value = delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type=ent_type)\n",
    "\n",
    "            phylo_tree  = 'Tree:' + str(indx) + '_' + phylo_tree\n",
    "            dic_final[phylo_tree] = delta_value\n",
    "\n",
    "            indx += 1\n",
    "    \n",
    "    return dic_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3C_Trees_1.txt': 4.378748165973167, '3C_Trees_2.txt': 4.377400837738492}\n"
     ]
    }
   ],
   "source": [
    "path_tree  = r\"./input/3Class/Trees\"\n",
    "\n",
    "Delta_Final = multiple_files_PastML(path_tree, path_data, single_tree_file=False)\n",
    "\n",
    "print( Delta_Final )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tree:0_((((Tupaia_chinensis:0.213873,((Castor_canadensis:0.154813,(Cricetulus_griseus:0.042672,Mus_musculus:0.066519):0.092299):0.032443,(Otolemur_garnettii:0.062095,(Cebus_capucinus:0.033412,((Homo_sapiens:0.025815,Nomascus_leucogenys:0.022456):0.012367,(Colobus_angolensis:0.037455,(Chlorocebus_sabaeus:0.021327,(Macaca_fascicularis:0.023317,Papio_anubis:0.00804):0.001585):0.016454):0.011472):0.007454):0.04107):0.023759):0.007001):0.028219,(((((Bos_mutus:0.023785,(Odocoileus_virginianus_texanus:0.02945,Capra_hircus:0.044736):0.009):0.116504,Sus_scrofa:0.073018):0.01157,(Camelus_bactrianus:0.084377,(Physeter_catodon:0.005252,Orcinus_orca:0.023933):0.03248):0.017824):0.029665,(Eptesicus_fuscus:0.07616,(Ceratotherium_simum_simum:0.047185,Equus_asinus:0.044757):0.013468):0.002871):0.008237,(Rhinolophus_sinicus:0.091862,(((Enhydra_lutris_kenyoni:0.075847,(Odobenus_rosmarus_divergens:0.050046,Ailuropoda_melanoleuca:0.035965):0.00886):0.014454,Canis_familiaris:0.066567):0.020043,Panthera_pardus:0.044653):0.025746):0.014115):0.016619):0.011688,Orycteropus_afer_afer:0.037785):0,Trichechus_manatus_latirostris:0.041417);': 4.380917536095901, 'Tree:1_((((Tupaia_chinensis:0.213873,((Castor_canadensis:0.154813,(Cricetulus_griseus:0.042672,Mus_musculus:0.066519):0.092299):0.032443,(Otolemur_garnettii:0.062095,(Cebus_capucinus:0.033412,((Homo_sapiens:0.025815,Nomascus_leucogenys:0.022456):0.012367,(Colobus_angolensis:0.037455,(Chlorocebus_sabaeus:0.021327,(Macaca_fascicularis:0.023317,Papio_anubis:0.00804):0.001585):0.016454):0.011472):0.007454):0.04107):0.023759):0.007001):0.028219,(((((Bos_mutus:0.023785,(Odocoileus_virginianus_texanus:0.02945,Capra_hircus:0.044736):0.009):0.116504,Sus_scrofa:0.073018):0.01157,(Camelus_bactrianus:0.084377,(Physeter_catodon:0.005252,Orcinus_orca:0.023933):0.03248):0.017824):0.029665,(Eptesicus_fuscus:0.07616,(Ceratotherium_simum_simum:0.047185,Equus_asinus:0.044757):0.013468):0.002871):0.008237,(Rhinolophus_sinicus:0.091862,(((Enhydra_lutris_kenyoni:0.075847,(Odobenus_rosmarus_divergens:0.050046,Ailuropoda_melanoleuca:0.035965):0.00886):0.014454,Canis_familiaris:0.066567):0.020043,Panthera_pardus:0.044653):0.025746):0.014115):0.016619):0.011688,Orycteropus_afer_afer:0.037785):0,Trichechus_manatus_latirostris:0.041417);': 4.373702058298747}\n",
      "dict_values([4.380917536095901, 4.373702058298747])\n"
     ]
    }
   ],
   "source": [
    "path_tree  = r\"./input/3Class/3C_Multiple_Trees.txt\"\n",
    "\n",
    "Delta_Final = multiple_files_PastML(path_tree, path_data, single_tree_file=True)\n",
    "\n",
    "print( Delta_Final )\n",
    "print( Delta_Final.values() )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                E.g. C1. (Single) rpy2:ape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small> <p> &ensp; PS. Before starting don't forget to install ape with the variable \"install_ape = True\" and select a CRAN mirror. </small> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tree = \"./input/2Class/Trees/2C_Trees_1.txt\"\n",
    "path_data = \"./input/2Class/2C_States.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b><p style=\"color:orange\";>ATENTION:</p></b>\n",
    "<p> You can select any \"Secure CRAN mirror\" but it is a good practice to choose a mirror closer to your location or one that is known to be reliable. CRAN (Comprehensive R Archive Network) is a network of servers worldwide that distribute R packages. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2866751261340254\n"
     ]
    }
   ],
   "source": [
    "Ancest_Prob = np.asarray( to_ape(path_tree, path_data, instal_ape=True, tree_dir=True) )\n",
    "Delta_Final = delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type='LSE')\n",
    "\n",
    "print( Delta_Final )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                E.g. C2. (Multiple) rpy2:ape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_files_Ape( dir_tree=None, dir_data=None, single_tree_file=False, lambda0=0.1, se=0.5, sim=100000, thin=10, burn=100, ent_type='LSE', install_ape=False ):\n",
    "    '''dir_tree      (default: None)   = Represents the directory path to the phylogenetic tree(s) file(s). If no value is provided when calling the function, the user will be prompted to input the directory path.\n",
    "    dir_data         (default: None)   = Represents the directory path to the observed states file(s). If no value is provided when calling the function, the user will be prompted to input the directory path.\n",
    "    single_tree_file (default: False)  = Boolean value that indicates whether the phylogenetic tree(s) file(s) are stored in a single file (True) or multiple files (False).\n",
    "    lambda0          (default: 0.1)    = A constant value used in the acceptance ratio computations.\n",
    "    se               (default: 0.5)    = The standard deviation used for the random walk in the Metropolis-Hastings algorithm.\n",
    "    sim              (default: 100000) = The number of total iterations in the Markov Chain Monte Carlo (MCMC) simulation.\n",
    "    thin             (default: 10)     = The thinning parameter, i.e., the number of iterations to discard between saved samples.\n",
    "    burn             (default: 100)    = The number of burn-in iterations to discard at the beginning of the simulation.\n",
    "    ent_type         (default: 'LSE')  = A string specifying the type of entropy calculation (options: 'LSE', 'SE', or any other value for Gini impurity).\n",
    "    instal_ape       (default: False)  = Boolean value that determines whether to install the 'ape' package or not.'''\n",
    "\n",
    "    if dir_tree == None:\n",
    "        dir_tree = str(input( 'What is the directory path to the phylogenetic tree(s) file(s)?' ))\n",
    "    if dir_data == None:\n",
    "        dir_data = str(input( 'What is the directory path to the observed states file(s)?' ))\n",
    "    \n",
    "    if install_ape==True:\n",
    "        r(\n",
    "        \"\"\"\n",
    "        # Instal Ape\n",
    "        install.packages('ape')\n",
    "        \"\"\")\n",
    "\n",
    "    if single_tree_file==False:\n",
    "        dic_files = set( os.listdir( dir_tree ) )\n",
    "        dic_delta = set()\n",
    "        for phylo_tree in dic_files:\n",
    "            dir_file     = np.loadtxt( dir_tree + '/' + phylo_tree, dtype=str )\n",
    "            Ancest_Prob  = np.asarray( to_ape( dir_file, dir_data, instal_ape=False ) )\n",
    "            dic_delta.add( delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type=ent_type) )\n",
    "        dic_final = dict(zip( dic_files, dic_delta ))\n",
    "    \n",
    "    else:\n",
    "        dic_final = {}\n",
    "        arr       = np.loadtxt( dir_tree , dtype=str)\n",
    "        indx      = 0\n",
    "        for phylo_tree in arr:\n",
    "            Ancest_Prob = np.asarray( to_ape( phylo_tree, dir_data, instal_ape=False ) )\n",
    "            delta_value = delta(x=Ancest_Prob, lambda0=lambda0, se=se, sim=sim, burn=burn, thin=thin, ent_type=ent_type)\n",
    "\n",
    "            phylo_tree  = 'Tree:' + str(indx) + '_' + phylo_tree\n",
    "            dic_final[phylo_tree] = delta_value\n",
    "\n",
    "            indx += 1\n",
    "\n",
    "    return dic_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small> <p> &ensp; PS. Before starting don't forget to install ape with the variable \"install_ape = True\" and select a CRAN mirror. </small> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tree:0_(Orycteropus_afer_afer:0.037785,Trichechus_manatus_latirostris:0.041417,((Tupaia_chinensis:0.213873,((Castor_canadensis:0.154813,(Cricetulus_griseus:0.042672,Mus_musculus:0.066519):0.092299):0.032443,(Otolemur_garnettii:0.062095,(Cebus_capucinus:0.033412,((Homo_sapiens:0.025815,Nomascus_leucogenys:0.022456):0.012367,(Colobus_angolensis:0.037455,(Chlorocebus_sabaeus:0.021327,(Macaca_fascicularis:0.023317,Papio_anubis:0.008040):0.001585):0.016454):0.011472):0.007454):0.041070):0.023759):0.007001):0.028219,(((((Bos_mutus:0.023785,(Odocoileus_virginianus_texanus:0.029450,Capra_hircus:0.044736):0.009000):0.116504,Sus_scrofa:0.073018):0.011570,(Camelus_bactrianus:0.084377,(Physeter_catodon:0.005252,Orcinus_orca:0.023933):0.032480):0.017824):0.029665,(Eptesicus_fuscus:0.076160,(Ceratotherium_simum_simum:0.047185,Equus_asinus:0.044757):0.013468):0.002871):0.008237,(Rhinolophus_sinicus:0.091862,(((Enhydra_lutris_kenyoni:0.075847,(Odobenus_rosmarus_divergens:0.050046,Ailuropoda_melanoleuca:0.035965):0.008860):0.014454,Canis_familiaris:0.066567):0.020043,Panthera_pardus:0.044653):0.025746):0.014115):0.016619):0.011688);': 0.28738838049442167, 'Tree:1_(Orycteropus_afer_afer:0.037785,Trichechus_manatus_latirostris:0.041417,((Tupaia_chinensis:0.213873,((Castor_canadensis:0.154813,(Cricetulus_griseus:0.042672,Mus_musculus:0.066519):0.092299):0.032443,(Otolemur_garnettii:0.062095,(Cebus_capucinus:0.033412,((Homo_sapiens:0.025815,Nomascus_leucogenys:0.022456):0.012367,(Colobus_angolensis:0.037455,(Chlorocebus_sabaeus:0.021327,(Macaca_fascicularis:0.023317,Papio_anubis:0.008040):0.001585):0.016454):0.011472):0.007454):0.041070):0.023759):0.007001):0.028219,(((((Bos_mutus:0.023785,(Odocoileus_virginianus_texanus:0.029450,Capra_hircus:0.044736):0.009000):0.116504,Sus_scrofa:0.073018):0.011570,(Camelus_bactrianus:0.084377,(Physeter_catodon:0.005252,Orcinus_orca:0.023933):0.032480):0.017824):0.029665,(Eptesicus_fuscus:0.076160,(Ceratotherium_simum_simum:0.047185,Equus_asinus:0.044757):0.013468):0.002871):0.008237,(Rhinolophus_sinicus:0.091862,(((Enhydra_lutris_kenyoni:0.075847,(Odobenus_rosmarus_divergens:0.050046,Ailuropoda_melanoleuca:0.035965):0.008860):0.014454,Canis_familiaris:0.066567):0.020043,Panthera_pardus:0.044653):0.025746):0.014115):0.016619):0.011688);': 0.2872315888221397}\n",
      "dict_values([0.28738838049442167, 0.2872315888221397])\n"
     ]
    }
   ],
   "source": [
    "path_tree = './input/2Class/2C_Multiple_Trees.txt'\n",
    "\n",
    "Delta_Final = multiple_files_Ape( path_tree, path_data, single_tree_file=True)\n",
    "\n",
    "print( Delta_Final )\n",
    "print( Delta_Final.values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2C_Trees_1.txt': 0.28717465910637696, '2C_Trees_2.txt': 0.28737860140909477}\n",
      "dict_values([0.28717465910637696, 0.28737860140909477])\n"
     ]
    }
   ],
   "source": [
    "path_tree = './input/2Class/Trees/'\n",
    "\n",
    "Delta_Final = multiple_files_Ape( path_tree, path_data, single_tree_file=False)\n",
    "\n",
    "print( Delta_Final )\n",
    "print( Delta_Final.values() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
